# Implementation of Show, Attend and Tell: Neural Image Caption Generation with Visual Attention paper
## COCO Image Captioning Dataset

The main objective of implementing Show, Attend and Tell: Neural Image Caption Generation with Visual Attention paper to generate image captions.
Captions are generated by the proposed CNN-LSTM network, in order to capture multiple objects inside an image, features are extracted from the lower convolutional layers.

We used four transfer learning networks for feature extraction:
- Visual Geometry Group (VGG)
- Residual neural network (ResNet)
- MobileNet
- Inception V3
